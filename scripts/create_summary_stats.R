#' ---
#' title:  create_summary_stats.R
#' author: Sebastian Mueller (sebm_at_posteo.de)
#' date:   2018-11-05
#' desc: Creating various summary statistics for barcodes (pre and post filtered)

# o   A delimited file containing information for each STAMP (before cut-off) on number of UMIs, number of Genes detected/captured and the number of NGS-reads
# o   A separate delimited file containing information for each STAMP (after cut-off) on number of UMIs, number of Genes detected/captured and the number of NGS-reads
# Example of the format could be as follows:
# STAMP id          Number of NGS-reads       Number of UMIs       Number of Genes Detected
# STAMP1           1000000                           50000                       6000
#' ---

#------------------------------------ for debuging:
# add the following line in config.yaml (without the #)
# DEBUG: True
# This will create R objects in the debug directory containing the snakemake object
#  R object that can be loaded into a custom R session as below:
# load("debug/snakemake_create_summary_stats.rdata")
# load(file="debug/R_image_create_summary_stats.rdata")

if (snakemake@config$DEBUG) {
  message("In debug mode: saving R objects to inspect later")
  dir.create("debug")
  save(snakemake, file = file.path("debug","snakemake_create_summary_stats.rdata"))
}
####/debug

library(dplyr) # Dataframe manipulation
library(Matrix) # Sparse matrices
library(stringr)
library(RColorBrewer)
library(devtools)
library(Seurat)
library(plotly)

samples <- snakemake@params$sample_names
batches <- snakemake@params$batches


# creating environment so objects don't get overwritten upon loading
env_imported_r_objects <- new.env()
# importing Seurat object
load(file = file.path(snakemake@input$R_objects),
     envir = env_imported_r_objects)
# attach
seuratobj <- env_imported_r_objects$seuratobj
meta.data <- seuratobj@meta.data

gini_index <- function (x, weights = rep(1, length = length(x))) {
    ox      <- order(x)
    x       <- x[ox]
    weights <- weights[ox] / sum(weights)
    p       <- cumsum(weights)
    nu      <- cumsum(weights * x)
    n       <- length(nu)
    nu      <- nu/nu[n]
    sum(nu[-1] * p[-n]) - sum(nu[-n] * p[-1])
}

#------------------------------------ post-filter-stats
# stats only based after keeping most abundant `expected-cells`
# taken out from seurat object generated in violine_plot rule.

#median calculator
stats_post <- meta.data %>%
  group_by(orig.ident) %>%
  summarise(mean_number_genes       = round(mean(nGene),2),
            sum_Counts              = sum(nCounts),
            sum_UMIs                = sum(nUMI),
            mean_Counts_per_STAMP   = round(mean(nCounts),2),
            mean_UMIs_per_STAMP     = round(mean(nUMI),2),
            mean_UMI_per_Gene       = round(mean(umi.per.gene),2),
            mean_Ribo_pct           = round(100 * mean(pct.Ribo),2),
            mean_Mito_pct           = round(100 * mean(pct.mito),2),
            read_length             = mean(read_length), # should be all the same anyway..
            expected_cells          = mean(expected_cells), # should be all the same anyway..
            actual_number_barcodes  = n())



# highest, lowest count/UMI Stamp
# pre STAMP stats

# hist out goes into knee plots
		# 'logs/{sample}_hist_out_cell.txt'
		# """export _JAVA_OPTIONS=-Djava.io.tmpdir={params.temp_directory} && BAMTagHistogram -m {params.memory}\
		# TAG=XC\
# https://hpc.nih.gov/apps/dropseq.html
# there is not hint in documentation on any filtering (only read quality)

#------------------------------------ pre-filter-stats
# calculating statistics based on barcodes before thresholding it (i.e. keeping the most abudant barcodes based on `expected cells`)
# This is based on 'logs/{sample}_hist_out_cell.txt' generated by `BAMTagHistogram` from dropseq-tools
# TOOO: The documentation only mentions duplicate and quality filter. But it seems do more filtering since most barcodes are expected to have only one read assinged but there are usually more. Find out.
# https://hpc.nih.gov/apps/dropseq.html

# TODO: Nr UMI for pre filter

stats_pre <- data.frame(matrix(nrow=length(samples), ncol=10))
colnames(stats_pre) <- c(
                    "Sample",
                    "Batch",
                    "Total_reads",
                    "Nr_barcodes_total",
                    "Nr_barcodes_more_than_1_reads",
                    "Nr_barcodes_more_than_10_reads",
                    "percentile99",
                    "percentile95",
                    "percentile50",
                    "Gini-index"
                    )

stats_pre[, "Sample"] <- samples
stats_pre[, "Batch"]  <- batches

for (i in 1:length(samples)){
  # importing 'logs/{sample}_hist_out_cell.txt'
  hist_out <- read.table(file = snakemake@input$hist_cell[i],
                         header = FALSE, stringsAsFactors = FALSE)
  sample <- samples[i]
  reads        <- hist_out$V1
  barcodes     <- hist_out$V2
  # calculations on reads
  total_reads  <- sum(reads)
  reads_cumsum <- cumsum(reads)
  reads_cumsum_perc <- (reads_cumsum/total_reads)
  # reporting stats
  stats_pre[i, "Total_reads"] <-  total_reads
  stats_pre[i, "Nr_barcodes_total"] <-  length(barcodes)
  stats_pre[i, "percentile99"] <-  which.min(reads_cumsum_perc<0.99)
  stats_pre[i, "percentile95"] <-  which.min(reads_cumsum_perc<0.95)
  stats_pre[i, "percentile50"] <-  which.min(reads_cumsum_perc<0.50)
  stats_pre[i, "Nr_barcodes_more_than_1_reads"]  <-  sum(reads > 1)
  stats_pre[i, "Nr_barcodes_more_than_10_reads"] <-  sum(reads > 10)
  stats_pre[i, "Gini-index"] <-  round(gini_index(reads),2)
  stats_post[i,"Pct_reads_after_filter"] <- round(100 * (stats_post[i, "sum_Counts"]  /
                                                              stats_pre [i, "Total_reads"]), 2)
}

# output
write.csv(stats_pre, file.path(snakemake@output$stats_pre))
write.csv(stats_post, file.path(snakemake@output$stats_post)) #writes table for excel


if (snakemake@config$DEBUG) {
 save.image(file = file.path("debug", "R_image_create_summary_stats.rdata"))
}
