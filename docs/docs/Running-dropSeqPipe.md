# Running the pipeline

Once everything is in place, the pipeline can be run using the snakemake command plus appropriate parameter.
It is recommended to take a [look at the options](http://snakemake.readthedocs.io/en/latest/) that are available since only a few of them will be covered below.

## The pipeline modes

* `meta`: Downloads and generates all the subsequent references files and STAR index needed to run the pipeline. Run this alone to just create the meta-data file before running a new set of data.
* `qc`: Creates fastqc reports of the data.
* `filter`: Generates `sample_filtered.fastq.gz` files from  `sample_R1.fastq.gz` files which are ready to be mapped to the genome.  This step filters out data for low quality reads and trims the adapters provided in the FILTER section.
* `map`: Mapping the reads (filtered fastq) to the reference. Goes from sample_filtered.fastq.gz to `sample_final.bam`.
* `extract`: Extracts the expression data such as a UMI and a count expression matrix.

There are two main ways to run the pipeline.
Either using a single command line that will run the whole pipeline or run each mode separately.
The second approach is especially recommended when working on a new protocol.
Since v`0.4` the pipeline detects mixed experiments if two instead of one reference is specified in `config.yaml`.
Note, the stepwise approach is not available for mixed experiments.

For both ways, one needs to first navigate to the dSP install directory (which contains the `Snakefile`).

## Running all modes in a single command
Run `snakemake all --use-conda --directory WORKING_DIR` which will run everything without stopping.


## Running each mode separately
The main advantage of the second way is that parameters can be fine tune based on the results of individual steps such as fastqc, filtering and mapping quality.

There are seven different modes available and call the mode to run it specifically.

Example:
To run the `qc` mode:
```
snakemake --cores 8 qc --use-conda --directory WORKING_DIR
```
To run multiple modes at the same time:
```
snakemake --cores 8 qc filter --use-conda --directory WORKING_DIR
```


## Useful snakemake parameters

* `--cores N` Use this argument to use X amount of cores available.
* `--notemp` Use this command to keep all temporary files. Without this option, only files between steps are kept. This option is useful to troubleshoot the pipeline or to analyze in between files.
* `--dryrun` or `-n` Use this to perform a dry run of the command. This is useful to check for potential missing files.
* `-p` Print out the shell commands that will be executed.
* `--verbose` Print debugging output.


## Software environments

### conda

If `--use-conda` is specified, snakemake downloads all necessary software in the working directory (in the hidden folder .snakemake).

By default, the .snakemake directory is created for each working directory (i.e. experiment), consequently downloading the entire software environments for each experiment.To avoid this,  a central conda location can be specified with `--conda-prefix dir` e.g.:

snakemake â€“use-conda --conda-prefix ~/.conda/myevns

We highly recommend this step as do [others](https://bioinformatics.stackexchange.com/questions/6914/running-snakemake-in-one-single-conda-env)

*Note: Its likely to run into problems when using `--use-cond` when running the pipeline while being in active conda environment.
Deactivate `conda` beforehand, if in doubt use `conda deactivate`.

### Singularity

Since version 0.5 Singularity is supported. A container with all software is now provided and automatically used if `--use-singularity` is used.

Again, this can be shared with other datasets using shared directory `--singularity-prefix ~/.singularity/`

Note: This is still immature, and some errors might be solved by specifying the reference-dir as an argument like so:
`--singularity-args="--bind path/to/reference-dir"`

The collection of container can be found here:

https://singularity-hub.org/collections/3535

## Folder Structure

This is the folder structure in the end:
```
/path/to/your/WORKING_DIR/
| -- RAW_DATA/
| -- RESULT_DIR/
| -- -- logs/
| -- -- -- cluster/
| -- -- plots/
| -- -- reports/
| -- -- summary/
| -- -- samples/
| samples.csv
| config.yaml
| barcodes.csv
| adapter.fa
| .snakemake/
```

* `RAW_DATA/` Contains all the samples as well as the intermediary files
* `RESULT_DIR/logs/` Contains all the logfiles generated by the pipeline
* `RESULT_DIR/logs/cluster` Contains all the logfiles generated by the cluster
* `RESULT_DIR/plots/` Contains all the plots generated by the pipeline
* `RESULT_DIR/reports/` Contains all the reports generated by the pipeline
* `RESULT_DIR/summary/` Contains all the files which might be used for downstream analysis (barcodes selected per sample and per species, final umi/counts expression matrix)
* `RESULT_DIR/samples/` Contains all the sample specific files, such as bam files, barcodes used and single sample expression files.
* `samples.csv` File containing sample details
* `config.yaml` File containing pipeline parameters as well as system parameters
* `adapters.fa` File containing all the adapters for trimming from raw data.
* `.snakemake/` Folder that contains amongst other things all the environments created for the run.

## testrun

A minimal test dataset for testing is provided with the pipeline but needs to be initialized first (in dSP directory):

```
git submodule update --init --recursive
```

Sets up test data in the `.test` dir. The pipeline can be tested by this minimal command:

```
snakemake --dir .test
```

or adding some sensible options:

```
snakemake --keep-going --use-conda -rp --conda-prefix ~/.conda/myevns --directory .tes
t
```
